{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84795,"databundleVersionId":10462807,"sourceType":"competition"},{"sourceId":162798,"sourceType":"modelInstanceVersion","modelInstanceId":138435,"modelId":161088}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":22.341371,"end_time":"2024-12-11T03:22:13.479076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-11T03:21:51.137705","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"import io\nimport os\nimport shutil\nimport json\nimport pandas as pd\nimport base64\n\nimport kaggle_evaluation.konwinski_prize_inference_server\n\n# Initialize global variables\ninstance_count = None\nfirst_prediction = True\nrepo_dataframe = None  # Global variable to store the DataFrame\n\ndef get_number_of_instances(num_instances: int) -> None:\n    \"\"\"\n    The very first message from the gateway will be the total number of instances to be served.\n    You don't need to edit this function.\n    \"\"\"\n    global instance_count\n    instance_count = num_instances\n\ndef predict(problem_statement: str, repo_archive: io.BytesIO) -> str:\n    \"\"\"\n    Inference function to read the repository archive into a pandas DataFrame.\n\n    Args:\n        problem_statement: The text of the git issue.\n        repo_archive: A BytesIO buffer containing a .tar archive of the codebase.\n\n    Returns:\n        A JSON string representing the DataFrame with repository contents.\n    \"\"\"\n    global first_prediction, repo_dataframe\n\n    if not first_prediction:\n        return None  # Skip processing if it's not the first prediction.\n\n    try:\n        # Step 1: Write the uploaded repository archive to a file\n        archive_filename = 'repo_archive.tar'\n        with open(archive_filename, 'wb') as f:\n            f.write(repo_archive.read())\n        print(f\"Successfully wrote the archive to '{archive_filename}'.\")\n\n        # Step 2: Define the extraction directory\n        repo_path = 'repo'\n\n        # Step 3: Remove the extraction directory if it already exists to ensure a clean state\n        if os.path.exists(repo_path):\n            shutil.rmtree(repo_path)\n            print(f\"Removed existing directory '{repo_path}/'.\")\n\n        # Step 4: Extract the repository archive to the specified directory\n        try:\n            shutil.unpack_archive(archive_filename, extract_dir=repo_path)\n            print(f\"Successfully extracted the archive to '{repo_path}/'.\")\n        except shutil.ReadError as e:\n            error_message = f\"Error unpacking archive: {e}\"\n            print(error_message)\n            return json.dumps({\"error\": error_message})\n\n        # Step 5: Remove the archive file after extraction to save space\n        os.remove(archive_filename)\n        print(f\"Removed the archive file '{archive_filename}'.\")\n\n        # Step 6: Initialize a list to hold repository data\n        data = []\n\n        # Step 7: Walk through the repository directory to read files\n        for root, dirs, files in os.walk(repo_path):\n            for file in files:\n                file_full_path = os.path.join(root, file)  # Get the full file path\n                relative_path = os.path.relpath(file_full_path, repo_path)\n                try:\n                    # Attempt to read the file content as text\n                    with open(file_full_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                    # Append the file path, content, and is_binary flag to the data list\n                    data.append({\n                        'file_path': relative_path,\n                        'content': content,\n                        'is_binary': False\n                    })\n                except UnicodeDecodeError:\n                    # If a UnicodeDecodeError occurs, treat the file as binary\n                    try:\n                        with open(file_full_path, 'rb') as f:\n                            binary_content = f.read()\n                        # Encode the binary content using Base64 to include in JSON\n                        encoded_content = base64.b64encode(binary_content).decode('utf-8')\n                        data.append({\n                            'file_path': relative_path,\n                            'content': encoded_content,\n                            'is_binary': True\n                        })\n                        print(f\"Encoded binary file '{file_full_path}'.\")\n                    except Exception as e:\n                        # If reading as binary also fails, note the error\n                        data.append({\n                            'file_path': relative_path,\n                            'content': f\"Could not read file: {e}\",\n                            'is_binary': None\n                        })\n                        print(f\"Could not read file '{file_full_path}': {e}\")\n                except Exception as e:\n                    # Handle other exceptions\n                    data.append({\n                        'file_path': relative_path,\n                        'content': f\"Could not read file: {e}\",\n                        'is_binary': None\n                    })\n                    print(f\"Could not read file '{file_full_path}': {e}\")\n\n        # Step 8: Create a pandas DataFrame from the collected data\n        repo_df = pd.DataFrame(data)\n        print(\"Successfully created the DataFrame from repository contents.\")\n\n        # Step 9: Convert the DataFrame to a JSON string\n        repo_json = repo_df.to_json(orient='records', indent=2)\n        print(\"Converted the DataFrame to JSON.\")\n\n        # Store the DataFrame in the global variable\n        repo_dataframe = repo_df\n        print(\"Stored the DataFrame in the global variable 'repo_dataframe'.\")\n\n        # Update the prediction flag\n        first_prediction = False\n\n        # Return the JSON string\n        return repo_json\n\n    except Exception as e:\n        # Handle unexpected exceptions and return as JSON error\n        error_response = {\n            \"error\": str(e)\n        }\n        print(f\"An unexpected error occurred: {e}\")\n        first_prediction = False\n        return json.dumps(error_response)\n\n# Initialize the inference server\ninference_server = kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n    get_number_of_instances,   \n    predict\n)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/konwinski-prize/',  # Path to the entire competition dataset\n            '/kaggle/tmp/konwinski-prize/',   # Path to a scratch directory for unpacking data.a_zip.\n        )\n    )\n\n# After the inference server has processed the predictions\nif repo_dataframe is not None:\n    # Perform operations on the DataFrame\n    print(repo_dataframe.head())\n    # You can also save it to a file if needed\n    repo_dataframe.to_csv('repository_contents.csv', index=False)\nelse:\n    print(\"The DataFrame 'repo_dataframe' is not available.\")\n","metadata":{}},{"cell_type":"code","source":"!pip install networkx plotly ray\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:19.661497Z","iopub.execute_input":"2024-12-30T19:10:19.661751Z","iopub.status.idle":"2024-12-30T19:10:28.263150Z","shell.execute_reply.started":"2024-12-30T19:10:19.661724Z","shell.execute_reply":"2024-12-30T19:10:28.262251Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.3)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.22.0)\nRequirement already satisfied: ray in /opt/conda/lib/python3.10/site-packages (2.24.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (21.3)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray) (3.15.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray) (4.22.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray) (1.0.8)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray) (1.4.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray) (2.32.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (0.18.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (2024.6.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os \nimport subprocess\n\nsplits = {'dev': 'data/dev-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'train': 'data/train-00000-of-00001.parquet'}\ndf = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"dev\"])\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:28.264499Z","iopub.execute_input":"2024-12-30T19:10:28.264769Z","iopub.status.idle":"2024-12-30T19:10:30.558524Z","shell.execute_reply.started":"2024-12-30T19:10:28.264742Z","shell.execute_reply":"2024-12-30T19:10:30.557702Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                  repo              instance_id  \\\n0    sqlfluff/sqlfluff  sqlfluff__sqlfluff-4764   \n1    sqlfluff/sqlfluff  sqlfluff__sqlfluff-2862   \n2    sqlfluff/sqlfluff  sqlfluff__sqlfluff-2336   \n3    sqlfluff/sqlfluff  sqlfluff__sqlfluff-5074   \n4    sqlfluff/sqlfluff  sqlfluff__sqlfluff-3436   \n..                 ...                      ...   \n220    pydicom/pydicom     pydicom__pydicom-809   \n221    pydicom/pydicom     pydicom__pydicom-933   \n222    pydicom/pydicom    pydicom__pydicom-1633   \n223    pydicom/pydicom    pydicom__pydicom-1428   \n224    pydicom/pydicom    pydicom__pydicom-1256   \n\n                                  base_commit  \\\n0    a820c139ccbe6d1865d73c4a459945cd69899f8f   \n1    447ecf862a4d2b977d0add9f444655357b9c4f1f   \n2    37a993f7ad841ab3035d1db5ce6525f2e5584fd5   \n3    7b7fd603a19755a9f3707ebbf95d18ee635716d8   \n4    23cd31e77a712a210c734e38488d7a34afd83a25   \n..                                        ...   \n220  356a51ab4bc54fd18950041ebc44dbfa1a425a10   \n221  38436b6824c079564b8760ea6acfa4c0fd3ee9c3   \n222  98ac88706e7ab17cd279c94949ac6af4e87f341d   \n223  674da68db47a71ee6929288a047b56cf31cf8168   \n224  49a3da4a3d9c24d7e8427a25048a1c7d5c4f7724   \n\n                                                 patch  \\\n0    diff --git a/src/sqlfluff/cli/commands.py b/sr...   \n1    diff --git a/src/sqlfluff/core/linter/common.p...   \n2    diff --git a/src/sqlfluff/core/rules/analysis/...   \n3    diff --git a/src/sqlfluff/core/errors.py b/src...   \n4    diff --git a/src/sqlfluff/core/templaters/slic...   \n..                                                 ...   \n220  diff --git a/pydicom/dataset.py b/pydicom/data...   \n221  diff --git a/pydicom/dataset.py b/pydicom/data...   \n222  diff --git a/pydicom/valuerep.py b/pydicom/val...   \n223  diff --git a/pydicom/fileset.py b/pydicom/file...   \n224  diff --git a/pydicom/jsonrep.py b/pydicom/json...   \n\n                                            test_patch  \\\n0    diff --git a/test/cli/commands_test.py b/test/...   \n1    diff --git a/test/api/simple_test.py b/test/ap...   \n2    diff --git a/test/core/rules/reference_test.py...   \n3    diff --git a/test/cli/commands_test.py b/test/...   \n4    diff --git a/test/core/templaters/jinja_test.p...   \n..                                                 ...   \n220  diff --git a/pydicom/tests/test_filewriter.py ...   \n221  diff --git a/pydicom/tests/test_filereader.py ...   \n222  diff --git a/pydicom/tests/test_valuerep.py b/...   \n223  diff --git a/pydicom/tests/test_fileset.py b/p...   \n224  diff --git a/pydicom/tests/test_json.py b/pydi...   \n\n                                     problem_statement  \\\n0    Enable quiet mode/no-verbose in CLI for use in...   \n1    fix keep adding new line on wrong place \\n### ...   \n2    L026: Rule incorrectly flag column does not ex...   \n3    Inconsistent output depending on --processes f...   \n4    Fatal templating error with Jinja templater. T...   \n..                                                 ...   \n220  \"Printing\" of certain dicom files fails once, ...   \n221  Deferred Read Fails For File-Like Objects\\n###...   \n222  OverflowError \"VR of 'DS' must be <= 16 charac...   \n223  Allow to search a list of elements in a `FileS...   \n224  from_json does not correctly convert BulkDataU...   \n\n                                            hints_text            created_at  \\\n0                                                       2023-04-16T14:24:42Z   \n1    > Version\\r\\n> sqlfluff, version 0.6.2\\r\\n\\r\\n...  2022-03-14T19:46:08Z   \n2                                                       2022-01-17T21:35:10Z   \n3    This is _very_ interesting! I'll pick this one...  2023-08-08T23:31:59Z   \n4    I'll take a look.\\r\\n\\r\\nAnd darn it -- first ...  2022-06-07T21:36:59Z   \n..                                                 ...                   ...   \n220  Occurs because Pixel Representation is in the ...  2019-03-04T20:14:54Z   \n221  This certainly makes sense, though deferred re...  2019-08-15T20:21:09Z   \n222  For reference, a possibly similar issue came u...  2022-04-14T18:26:56Z   \n223  Sounds good, do you want to do the PR? Just ch...  2021-06-28T08:57:19Z   \n224                                                     2020-11-04T21:13:33Z   \n\n    version                                       FAIL_TO_PASS  \\\n0       1.4  [\"test/cli/commands_test.py::test__cli__fix_mu...   \n1      0.10  [\"test/api/simple_test.py::test__api__lint_str...   \n2       0.8  [\"test/core/rules/reference_test.py::test_obje...   \n3       2.1  [\"test/cli/commands_test.py::test__cli__comman...   \n4      0.13  [\"test/core/templaters/jinja_test.py::test__te...   \n..      ...                                                ...   \n220     1.2  [\"pydicom/tests/test_filewriter.py::TestCorrec...   \n221     1.3  [\"pydicom/tests/test_filereader.py::TestDeferr...   \n222     2.3  [\"pydicom/tests/test_valuerep.py::TestDSfloat:...   \n223     2.1  [\"pydicom/tests/test_fileset.py::TestFileSet_L...   \n224     2.1  [\"pydicom/tests/test_json.py::TestBinary::test...   \n\n                                          PASS_TO_PASS  \\\n0    [\"test/cli/commands_test.py::test__cli__comman...   \n1    [\"test/api/simple_test.py::test__api__lint_str...   \n2                                                   []   \n3    [\"test/cli/commands_test.py::test__cli__comman...   \n4    [\"test/core/templaters/jinja_test.py::test__te...   \n..                                                 ...   \n220  [\"pydicom/tests/test_filewriter.py::TestWriteF...   \n221  [\"pydicom/tests/test_filereader.py::TestReader...   \n222  [\"pydicom/tests/test_valuerep.py::TestTM::test...   \n223  [\"pydicom/tests/test_fileset.py::test_is_confo...   \n224  [\"pydicom/tests/test_json.py::TestPersonName::...   \n\n                     environment_setup_commit  \n0    d19de0ecd16d298f9e3bfb91da122734c40c01e5  \n1    3d52e8270d82aeccf4c516d059a80a6947919aea  \n2    a5c4eae4e3e419fe95460c9afd9cf39a35a470c4  \n3    7b7fd603a19755a9f3707ebbf95d18ee635716d8  \n4    6e8ce43a4958dbaa56256365c2a89d8db92e07d6  \n..                                        ...  \n220  b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9  \n221  7241f5d9db0de589b230bb84212fbb643a7c86c3  \n222  a8be738418dee0a2b93c241fbd5e0bc82f4b8680  \n223  506ecea8f378dc687d5c504788fc78810a190b7a  \n224  506ecea8f378dc687d5c504788fc78810a190b7a  \n\n[225 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repo</th>\n      <th>instance_id</th>\n      <th>base_commit</th>\n      <th>patch</th>\n      <th>test_patch</th>\n      <th>problem_statement</th>\n      <th>hints_text</th>\n      <th>created_at</th>\n      <th>version</th>\n      <th>FAIL_TO_PASS</th>\n      <th>PASS_TO_PASS</th>\n      <th>environment_setup_commit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sqlfluff/sqlfluff</td>\n      <td>sqlfluff__sqlfluff-4764</td>\n      <td>a820c139ccbe6d1865d73c4a459945cd69899f8f</td>\n      <td>diff --git a/src/sqlfluff/cli/commands.py b/sr...</td>\n      <td>diff --git a/test/cli/commands_test.py b/test/...</td>\n      <td>Enable quiet mode/no-verbose in CLI for use in...</td>\n      <td></td>\n      <td>2023-04-16T14:24:42Z</td>\n      <td>1.4</td>\n      <td>[\"test/cli/commands_test.py::test__cli__fix_mu...</td>\n      <td>[\"test/cli/commands_test.py::test__cli__comman...</td>\n      <td>d19de0ecd16d298f9e3bfb91da122734c40c01e5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sqlfluff/sqlfluff</td>\n      <td>sqlfluff__sqlfluff-2862</td>\n      <td>447ecf862a4d2b977d0add9f444655357b9c4f1f</td>\n      <td>diff --git a/src/sqlfluff/core/linter/common.p...</td>\n      <td>diff --git a/test/api/simple_test.py b/test/ap...</td>\n      <td>fix keep adding new line on wrong place \\n### ...</td>\n      <td>&gt; Version\\r\\n&gt; sqlfluff, version 0.6.2\\r\\n\\r\\n...</td>\n      <td>2022-03-14T19:46:08Z</td>\n      <td>0.10</td>\n      <td>[\"test/api/simple_test.py::test__api__lint_str...</td>\n      <td>[\"test/api/simple_test.py::test__api__lint_str...</td>\n      <td>3d52e8270d82aeccf4c516d059a80a6947919aea</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sqlfluff/sqlfluff</td>\n      <td>sqlfluff__sqlfluff-2336</td>\n      <td>37a993f7ad841ab3035d1db5ce6525f2e5584fd5</td>\n      <td>diff --git a/src/sqlfluff/core/rules/analysis/...</td>\n      <td>diff --git a/test/core/rules/reference_test.py...</td>\n      <td>L026: Rule incorrectly flag column does not ex...</td>\n      <td></td>\n      <td>2022-01-17T21:35:10Z</td>\n      <td>0.8</td>\n      <td>[\"test/core/rules/reference_test.py::test_obje...</td>\n      <td>[]</td>\n      <td>a5c4eae4e3e419fe95460c9afd9cf39a35a470c4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sqlfluff/sqlfluff</td>\n      <td>sqlfluff__sqlfluff-5074</td>\n      <td>7b7fd603a19755a9f3707ebbf95d18ee635716d8</td>\n      <td>diff --git a/src/sqlfluff/core/errors.py b/src...</td>\n      <td>diff --git a/test/cli/commands_test.py b/test/...</td>\n      <td>Inconsistent output depending on --processes f...</td>\n      <td>This is _very_ interesting! I'll pick this one...</td>\n      <td>2023-08-08T23:31:59Z</td>\n      <td>2.1</td>\n      <td>[\"test/cli/commands_test.py::test__cli__comman...</td>\n      <td>[\"test/cli/commands_test.py::test__cli__comman...</td>\n      <td>7b7fd603a19755a9f3707ebbf95d18ee635716d8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqlfluff/sqlfluff</td>\n      <td>sqlfluff__sqlfluff-3436</td>\n      <td>23cd31e77a712a210c734e38488d7a34afd83a25</td>\n      <td>diff --git a/src/sqlfluff/core/templaters/slic...</td>\n      <td>diff --git a/test/core/templaters/jinja_test.p...</td>\n      <td>Fatal templating error with Jinja templater. T...</td>\n      <td>I'll take a look.\\r\\n\\r\\nAnd darn it -- first ...</td>\n      <td>2022-06-07T21:36:59Z</td>\n      <td>0.13</td>\n      <td>[\"test/core/templaters/jinja_test.py::test__te...</td>\n      <td>[\"test/core/templaters/jinja_test.py::test__te...</td>\n      <td>6e8ce43a4958dbaa56256365c2a89d8db92e07d6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>pydicom/pydicom</td>\n      <td>pydicom__pydicom-809</td>\n      <td>356a51ab4bc54fd18950041ebc44dbfa1a425a10</td>\n      <td>diff --git a/pydicom/dataset.py b/pydicom/data...</td>\n      <td>diff --git a/pydicom/tests/test_filewriter.py ...</td>\n      <td>\"Printing\" of certain dicom files fails once, ...</td>\n      <td>Occurs because Pixel Representation is in the ...</td>\n      <td>2019-03-04T20:14:54Z</td>\n      <td>1.2</td>\n      <td>[\"pydicom/tests/test_filewriter.py::TestCorrec...</td>\n      <td>[\"pydicom/tests/test_filewriter.py::TestWriteF...</td>\n      <td>b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>pydicom/pydicom</td>\n      <td>pydicom__pydicom-933</td>\n      <td>38436b6824c079564b8760ea6acfa4c0fd3ee9c3</td>\n      <td>diff --git a/pydicom/dataset.py b/pydicom/data...</td>\n      <td>diff --git a/pydicom/tests/test_filereader.py ...</td>\n      <td>Deferred Read Fails For File-Like Objects\\n###...</td>\n      <td>This certainly makes sense, though deferred re...</td>\n      <td>2019-08-15T20:21:09Z</td>\n      <td>1.3</td>\n      <td>[\"pydicom/tests/test_filereader.py::TestDeferr...</td>\n      <td>[\"pydicom/tests/test_filereader.py::TestReader...</td>\n      <td>7241f5d9db0de589b230bb84212fbb643a7c86c3</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>pydicom/pydicom</td>\n      <td>pydicom__pydicom-1633</td>\n      <td>98ac88706e7ab17cd279c94949ac6af4e87f341d</td>\n      <td>diff --git a/pydicom/valuerep.py b/pydicom/val...</td>\n      <td>diff --git a/pydicom/tests/test_valuerep.py b/...</td>\n      <td>OverflowError \"VR of 'DS' must be &lt;= 16 charac...</td>\n      <td>For reference, a possibly similar issue came u...</td>\n      <td>2022-04-14T18:26:56Z</td>\n      <td>2.3</td>\n      <td>[\"pydicom/tests/test_valuerep.py::TestDSfloat:...</td>\n      <td>[\"pydicom/tests/test_valuerep.py::TestTM::test...</td>\n      <td>a8be738418dee0a2b93c241fbd5e0bc82f4b8680</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>pydicom/pydicom</td>\n      <td>pydicom__pydicom-1428</td>\n      <td>674da68db47a71ee6929288a047b56cf31cf8168</td>\n      <td>diff --git a/pydicom/fileset.py b/pydicom/file...</td>\n      <td>diff --git a/pydicom/tests/test_fileset.py b/p...</td>\n      <td>Allow to search a list of elements in a `FileS...</td>\n      <td>Sounds good, do you want to do the PR? Just ch...</td>\n      <td>2021-06-28T08:57:19Z</td>\n      <td>2.1</td>\n      <td>[\"pydicom/tests/test_fileset.py::TestFileSet_L...</td>\n      <td>[\"pydicom/tests/test_fileset.py::test_is_confo...</td>\n      <td>506ecea8f378dc687d5c504788fc78810a190b7a</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>pydicom/pydicom</td>\n      <td>pydicom__pydicom-1256</td>\n      <td>49a3da4a3d9c24d7e8427a25048a1c7d5c4f7724</td>\n      <td>diff --git a/pydicom/jsonrep.py b/pydicom/json...</td>\n      <td>diff --git a/pydicom/tests/test_json.py b/pydi...</td>\n      <td>from_json does not correctly convert BulkDataU...</td>\n      <td></td>\n      <td>2020-11-04T21:13:33Z</td>\n      <td>2.1</td>\n      <td>[\"pydicom/tests/test_json.py::TestBinary::test...</td>\n      <td>[\"pydicom/tests/test_json.py::TestPersonName::...</td>\n      <td>506ecea8f378dc687d5c504788fc78810a190b7a</td>\n    </tr>\n  </tbody>\n</table>\n<p>225 rows × 12 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport subprocess\nimport pandas as pd\nimport tempfile\n\n\ndef is_binary_file(filepath):\n    \"\"\"Utility function to detect if a file is binary.\"\"\"\n    with open(filepath, 'rb') as file:\n        chunk = file.read(1024)\n        return b'\\0' in chunk\n\n\ndef analyze_repo_contents(df, index):\n    \"\"\"\n    Analyze repository contents before and after a patch and load them into separate DataFrames,\n    including an 'is_binary' field.\n    \n    Args:\n        df (pd.DataFrame): DataFrame containing SWE-bench data.\n        index (int): Index of the row to analyze.\n\n    Returns:\n        pd.DataFrame, pd.DataFrame: DataFrames for repository contents before and after the patch.\n    \"\"\"\n    if index >= len(df):\n        raise IndexError(\"The provided index is out of range.\")\n\n    # Extract relevant row\n    row = df.iloc[index]\n\n    # Extract repository information\n    repo_url = f\"https://github.com/{row['repo']}.git\"\n    repo_name = row['repo'].split('/')[-1]\n    repo_path = f\"./{repo_name}\"\n\n    # Clone the repository if not already cloned\n    if not os.path.exists(repo_path):\n        print(f\"Cloning repository {repo_url}...\")\n        subprocess.run([\"git\", \"clone\", repo_url, repo_path], check=True)\n    else:\n        print(f\"Repository {repo_name} already cloned.\")\n\n    # Checkout the base commit (before the patch)\n    base_commit = row['base_commit']\n    subprocess.run([\"git\", \"checkout\", base_commit], cwd=repo_path, check=True)\n\n    # Load repository contents before the patch\n    print(f\"Loading repository contents at base commit {base_commit}...\")\n    pre_patch_files = []\n    for root, dirs, files in os.walk(repo_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            is_binary = is_binary_file(file_path)\n            if is_binary:\n                with open(file_path, \"rb\") as f:  # Binary mode\n                    content = f.read()\n            else:\n                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:  # Text mode\n                    content = f.read()\n            pre_patch_files.append({\"file_path\": file_path, \"content\": content, \"is_binary\": is_binary})\n\n    pre_patch_df = pd.DataFrame(pre_patch_files)\n\n    # Save and attempt to apply the patch\n    patch = row['patch']\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".diff\") as temp_patch_file:\n        patch_file_path = temp_patch_file.name\n        temp_patch_file.write(patch.encode(\"utf-8\"))\n\n    try:\n        subprocess.run([\"git\", \"apply\", patch_file_path], cwd=repo_path, check=True)\n        print(\"Patch applied successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(\"Patch application failed.\")\n        print(f\"Error: {e}\")\n        print(\"Proceeding without applying the patch.\")\n    finally:\n        os.remove(patch_file_path)\n\n    # Load repository contents after attempting to apply the patch\n    print(\"Loading repository contents after patch attempt...\")\n    post_patch_files = []\n    for root, dirs, files in os.walk(repo_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            is_binary = is_binary_file(file_path)\n            if is_binary:\n                with open(file_path, \"rb\") as f:  # Binary mode\n                    content = f.read()\n            else:\n                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:  # Text mode\n                    content = f.read()\n            post_patch_files.append({\"file_path\": file_path, \"content\": content, \"is_binary\": is_binary})\n\n    post_patch_df = pd.DataFrame(post_patch_files)\n\n    return pre_patch_df, post_patch_df\n\n\n# Analyze the first row\npre_patch_df, post_patch_df = analyze_repo_contents(df, index=0)\n\n# View DataFrames\nprint(pre_patch_df.head())\nprint(post_patch_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:30.559810Z","iopub.execute_input":"2024-12-30T19:10:30.560192Z","iopub.status.idle":"2024-12-30T19:10:30.972700Z","shell.execute_reply.started":"2024-12-30T19:10:30.560165Z","shell.execute_reply":"2024-12-30T19:10:30.971772Z"}},"outputs":[{"name":"stdout","text":"Repository sqlfluff already cloned.\nM\tsrc/sqlfluff/cli/commands.py\nM\tsrc/sqlfluff/cli/formatters.py\nM\tsrc/sqlfluff/core/linter/linted_dir.py\nLoading repository contents at base commit a820c139ccbe6d1865d73c4a459945cd69899f8f...\n","output_type":"stream"},{"name":"stderr","text":"HEAD is now at a820c139c Use the new CollationReferenceSegment everywhere (#4770)\nerror: patch failed: src/sqlfluff/cli/commands.py:44\nerror: src/sqlfluff/cli/commands.py: patch does not apply\nerror: patch failed: src/sqlfluff/cli/formatters.py:94\nerror: src/sqlfluff/cli/formatters.py: patch does not apply\nerror: patch failed: src/sqlfluff/core/linter/linted_dir.py:117\nerror: src/sqlfluff/core/linter/linted_dir.py: patch does not apply\n","output_type":"stream"},{"name":"stdout","text":"Patch application failed.\nError: Command '['git', 'apply', '/tmp/tmp6h5jn9bm.diff']' returned non-zero exit status 1.\nProceeding without applying the patch.\nLoading repository contents after patch attempt...\n                   file_path  \\\n0  ./sqlfluff/.gitattributes   \n1   ./sqlfluff/.editorconfig   \n2     ./sqlfluff/MANIFEST.in   \n3         ./sqlfluff/tox.ini   \n4   ./sqlfluff/.dockerignore   \n\n                                             content  is_binary  \n0  # We'll let Git's auto-detection algorithm inf...      False  \n1  # editorconfig.org\\nroot = true\\n\\n[*]\\nindent...      False  \n2        include README.md LICENSE.md CHANGELOG.md\\n      False  \n3  [tox]\\nenvlist = generate-fixture-yml, linting...      False  \n4  # Ignore IDE files\\n.vscode\\n.idea\\n/.sqlfluff...      False  \n                   file_path  \\\n0  ./sqlfluff/.gitattributes   \n1   ./sqlfluff/.editorconfig   \n2     ./sqlfluff/MANIFEST.in   \n3         ./sqlfluff/tox.ini   \n4   ./sqlfluff/.dockerignore   \n\n                                             content  is_binary  \n0  # We'll let Git's auto-detection algorithm inf...      False  \n1  # editorconfig.org\\nroot = true\\n\\n[*]\\nindent...      False  \n2        include README.md LICENSE.md CHANGELOG.md\\n      False  \n3  [tox]\\nenvlist = generate-fixture-yml, linting...      False  \n4  # Ignore IDE files\\n.vscode\\n.idea\\n/.sqlfluff...      False  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Define mapping dictionaries\nextension_category_map = {\n    # Source Code\n    '.py': 'Source Code Files',\n    '.js': 'Source Code Files',\n    '.java': 'Source Code Files',\n    '.c': 'Source Code Files',\n    '.cpp': 'Source Code Files',\n    '.cs': 'Source Code Files',\n    '.rb': 'Source Code Files',\n    '.go': 'Source Code Files',\n    '.ts': 'Source Code Files',\n    '.php': 'Source Code Files',\n    '.swift': 'Source Code Files',\n    '.kt': 'Source Code Files',\n\n    # Configuration\n    '.cfg': 'Configuration Files',\n    '.toml': 'Configuration Files',\n    '.yaml': 'Configuration Files',\n    '.yml': 'Configuration Files',\n    '.json': 'Configuration Files',\n    '.ini': 'Configuration Files',\n    '.env': 'Configuration Files',\n    '.editorconfig': 'Configuration Files',\n    '.git-blame-ignore-revs': 'Version Control Configuration Files',\n    'pyproject.toml': 'Configuration Files',\n\n    # Documentation\n    '.md': 'Documentation Files',\n    '.rst': 'Documentation Files',\n    '.txt': 'Documentation Files',\n    '.adoc': 'Documentation Files',\n\n    # License and Legal\n    'LICENSE': 'License and Legal Files',\n    'LICENSE.txt': 'License and Legal Files',\n    'LICENSE.md': 'License and Legal Files',\n    'NOTICE': 'License and Legal Files',\n\n    # Scripts\n    '.sh': 'Scripts and Utilities',\n    '.bat': 'Scripts and Utilities',\n    '.ps1': 'Scripts and Utilities',\n    '.pyw': 'Scripts and Utilities',\n\n    # Testing\n    '.test': 'Testing Files',\n    '.spec': 'Testing Files',\n    '.pytest': 'Testing Files',\n    'pytest.ini': 'Testing Files',\n    'tox.ini': 'Testing Files',\n\n    # Build and Deployment\n    'Dockerfile': 'Build and Deployment Files',\n    '.dockerignore': 'Build and Deployment Files',\n    'Makefile': 'Build and Deployment Files',\n    'docker-compose.yml': 'Build and Deployment Files',\n    'Jenkinsfile': 'Build and Deployment Files',\n    'build.gradle': 'Build and Deployment Files',\n    'pom.xml': 'Build and Deployment Files',\n\n    # Version Control Configuration\n    '.gitignore': 'Version Control Configuration Files',\n    '.gitattributes': 'Version Control Configuration Files',\n    '.gitmodules': 'Version Control Configuration Files',\n\n    # Workflow and CI\n    '.travis.yml': 'Workflow and CI Files',\n    '.circleci/config.yml': 'Workflow and CI Files',\n    '.github/workflows/ci.yaml': 'Workflow and CI Files',\n    '.github/workflows/release-tests.yml': 'Workflow and CI Files',\n    '.github/workflows/release.yml': 'Workflow and CI Files',\n    '.github/workflows/codeql-analysis.yml': 'Workflow and CI Files',\n    '.github/workflows/backport.yml': 'Workflow and CI Files',\n    # Add more special files as needed\n}\n\n# Directory patterns mapped to categories\ndirectory_category_map = {\n    'docs/': 'Documentation Files',\n    'docs': 'Documentation Files',\n    'test/': 'Testing Files',\n    'tests/': 'Testing Files',\n    'assets/': 'Binary and Asset Files',\n    'static/': 'Binary and Asset Files',\n    'scripts/': 'Scripts and Utilities',\n    'bin/': 'Scripts and Utilities',\n    'config/': 'Configuration Files',\n    'src/': 'Source Code Files',\n    'lib/': 'Source Code Files',\n    'include/': 'Source Code Files',\n    'examples/': 'Examples and Demos',\n    'public/': 'Public Assets',\n    # Add more directory patterns as needed\n}\n\n# Binary file extensions\nbinary_extensions = {\n    '.png', '.jpg', '.jpeg', '.gif', '.svg', '.exe', '.dll', '.so',\n    '.bin', '.ico', '.pdf', '.zip', '.tar', '.gz', '.7z', '.rar',\n    '.mp3', '.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv', '.bmp',\n    '.tiff', '.woff', '.woff2', '.ttf', '.eot', '.otf', '.dmg',\n    '.iso', '.jar', '.war', '.ear'\n}\n\n# Function to classify files\ndef classify_file(row):\n    file_path = row['file_path']\n    is_binary = row['is_binary']\n    \n    # Normalize file_path for consistent matching\n    normalized_path = file_path.lower()\n    \n    # Check for exact matches first (e.g., Dockerfile, LICENSE)\n    if file_path in extension_category_map:\n        return extension_category_map[file_path]\n    \n    # Check directory patterns\n    for dir_pattern, category in directory_category_map.items():\n        if normalized_path.startswith(dir_pattern):\n            return category\n    \n    # Extract the file extension or specific filename\n    basename = os.path.basename(file_path)\n    _, ext = os.path.splitext(basename)\n    ext = ext.lower()\n    \n    # Check exact filename matches if not already matched\n    if basename in extension_category_map:\n        return extension_category_map[basename]\n    \n    # Check extension-based category\n    if ext in extension_category_map:\n        return extension_category_map[ext]\n    \n    # Check for binary files based on extension\n    if is_binary or ext in binary_extensions:\n        return 'Binary and Asset Files'\n    \n    # Default category\n    return 'Miscellaneous Files'\n\n# Apply classification\npre_patch_df['Category'] = pre_patch_df.apply(classify_file, axis=1)\npost_patch_df['Category'] = post_patch_df.apply(classify_file, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:30.974400Z","iopub.execute_input":"2024-12-30T19:10:30.974763Z","iopub.status.idle":"2024-12-30T19:10:31.063175Z","shell.execute_reply.started":"2024-12-30T19:10:30.974720Z","shell.execute_reply":"2024-12-30T19:10:31.062492Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pre_patch_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:31.064199Z","iopub.execute_input":"2024-12-30T19:10:31.064503Z","iopub.status.idle":"2024-12-30T19:10:31.070334Z","shell.execute_reply.started":"2024-12-30T19:10:31.064476Z","shell.execute_reply":"2024-12-30T19:10:31.069449Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['file_path', 'content', 'is_binary', 'Category'], dtype='object')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"pre_patch_df.iloc[:1, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:31.072795Z","iopub.execute_input":"2024-12-30T19:10:31.073068Z","iopub.status.idle":"2024-12-30T19:10:31.086954Z","shell.execute_reply.started":"2024-12-30T19:10:31.073042Z","shell.execute_reply":"2024-12-30T19:10:31.085919Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                   file_path  \\\n0  ./sqlfluff/.gitattributes   \n\n                                             content  is_binary  \\\n0  # We'll let Git's auto-detection algorithm inf...      False   \n\n                              Category  \n0  Version Control Configuration Files  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_path</th>\n      <th>content</th>\n      <th>is_binary</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./sqlfluff/.gitattributes</td>\n      <td># We'll let Git's auto-detection algorithm inf...</td>\n      <td>False</td>\n      <td>Version Control Configuration Files</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"pre_patch_df['Category'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:31.088105Z","iopub.execute_input":"2024-12-30T19:10:31.089033Z","iopub.status.idle":"2024-12-30T19:10:31.102715Z","shell.execute_reply.started":"2024-12-30T19:10:31.089002Z","shell.execute_reply":"2024-12-30T19:10:31.101923Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Category\nMiscellaneous Files                    1582\nConfiguration Files                    1428\nSource Code Files                       320\nDocumentation Files                      42\nBinary and Asset Files                   13\nVersion Control Configuration Files       6\nBuild and Deployment Files                4\nTesting Files                             4\nLicense and Legal Files                   2\nScripts and Utilities                     1\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"post_patch_df['Category'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:31.103826Z","iopub.execute_input":"2024-12-30T19:10:31.104138Z","iopub.status.idle":"2024-12-30T19:10:31.115514Z","shell.execute_reply.started":"2024-12-30T19:10:31.104112Z","shell.execute_reply":"2024-12-30T19:10:31.114722Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Category\nMiscellaneous Files                    1582\nConfiguration Files                    1428\nSource Code Files                       320\nDocumentation Files                      42\nBinary and Asset Files                   13\nVersion Control Configuration Files       6\nBuild and Deployment Files                4\nTesting Files                             4\nLicense and Legal Files                   2\nScripts and Utilities                     1\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Codebase Deep Dive**","metadata":{}},{"cell_type":"code","source":"import torch\n\nprint(\"GPUs available:\", torch.cuda.device_count())\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:31.116635Z","iopub.execute_input":"2024-12-30T19:10:31.116992Z","iopub.status.idle":"2024-12-30T19:10:32.767581Z","shell.execute_reply.started":"2024-12-30T19:10:31.116934Z","shell.execute_reply":"2024-12-30T19:10:32.766555Z"}},"outputs":[{"name":"stdout","text":"GPUs available: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import ray\n\nray.init(num_gpus=torch.cuda.device_count())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:12:39.860933Z","iopub.execute_input":"2024-12-30T19:12:39.861985Z","iopub.status.idle":"2024-12-30T19:12:43.449962Z","shell.execute_reply.started":"2024-12-30T19:12:39.861945Z","shell.execute_reply":"2024-12-30T19:12:43.448660Z"}},"outputs":[{"name":"stderr","text":"2024-12-30 19:12:42,009\tINFO worker.py:1753 -- Started a local Ray instance.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RayContext(dashboard_url='', python_version='3.10.14', ray_version='2.24.0', ray_commit='cfea8b29800afc76823bd1a171657010f4eb96bb')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699395ceba61431fbf1e9677f8c7798f"},"text/html":"<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n    <g clip-path=\"url(#clip0_4338_178347)\">\n        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n    </g>\n    <defs>\n        <clipPath id=\"clip0_4338_178347\">\n            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n        </clipPath>\n    </defs>\n  </svg>\n</div>\n\n        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n    <tr>\n        <td style=\"text-align: left\"><b>Python version:</b></td>\n        <td style=\"text-align: left\"><b>3.10.14</b></td>\n    </tr>\n    <tr>\n        <td style=\"text-align: left\"><b>Ray version:</b></td>\n        <td style=\"text-align: left\"><b>2.24.0</b></td>\n    </tr>\n    \n</table>\n\n    </div>\n</div>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[36m(DistributedModelWorker pid=1665)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DistributedModelWorker.__init__()\u001b[39m (pid=1665, ip=172.19.2.2, actor_id=58d00361ea79d9ee11bf6b3401000000, repr=<__main__.DistributedModelWorker object at 0x7bc235f85900>)\n\u001b[36m(DistributedModelWorker pid=1665)\u001b[0m   File \"/tmp/ipykernel_1009/2694572308.py\", line 476, in __init__\n\u001b[36m(DistributedModelWorker pid=1665)\u001b[0m   File \"/tmp/ipykernel_1009/2694572308.py\", line 288, in __init__\n\u001b[36m(DistributedModelWorker pid=1665)\u001b[0m ValueError: Invalid GPU ID: 1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport logging\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport ray\nfrom typing import List, Dict\nimport numpy as np\nimport sys\n\n# --------------------------- Configuration --------------------------- #\n\n# Model and Authentication\nMODEL_DIR = \"/kaggle/input/qwen2.5-coder/transformers/1.5b-instruct/1\"  # Update this path as needed\nHF_TOKEN = \"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\"  # Replace with your Hugging Face token\n\n# Output Files\nOUTPUT_FILE = \"complete_codebase_documentation.md\"  # Output Markdown file\nLOG_FILE = \"repo_inferencer.log\"  # Log file\n\n# Model Parameters\nMAX_TOKENS = 4096  # Maximum tokens for model generation\nCHUNK_SIZE = 2000  # Token-based chunk size to break the input\n\n# Prompt Templates for Different Categories\nPROMPT_TEMPLATES = {\n    \"Source Code\": {\n        \"summary\": \"\"\"You are an expert software developer tasked with analyzing and documenting source code. Follow these steps to provide a comprehensive summary:\n\nStep 1: Code Understanding\n- Carefully read through the provided source code\n- Identify the main purpose and functionality\n- Note any important dependencies or imports\n\nStep 2: Architecture Analysis\n- Identify key classes, functions, and modules\n- Understand the code structure and organization\n- Note any design patterns or architectural choices\n\nStep 3: Generate Summary\nProvide a clear summary addressing:\n1. Main purpose and functionality\n2. Key components and their roles\n3. Important dependencies\n4. Notable design patterns or architectural decisions\n5. Any performance considerations\n\nFile: {file_path}\n\nCode:\n{content}\n\nSummary:\"\"\",\n        \"key_points\": \"\"\"As an expert developer, extract key technical points from this code by following these steps:\n\nStep 1: Technical Analysis\n- Review the code's technical implementation\n- Identify core algorithms and data structures\n- Note any important configurations or settings\n\nStep 2: Implementation Details\n- List critical functions and their purposes\n- Document important class relationships\n- Note any complex logic or algorithms\n\nStep 3: Identify Key Technical Points\nFocus on:\n1. Critical functions and their roles\n2. Important class hierarchies\n3. Key algorithms and data structures\n4. Configuration parameters\n5. Error handling mechanisms\n6. Performance optimizations\n\nFile: {file_path}\n\nCode:\n{content}\n\nKey Technical Points:\"\"\"\n    },\n    \"Configuration\": {\n        \"summary\": \"\"\"As a configuration specialist, analyze this configuration file following these steps:\n\nStep 1: Configuration Analysis\n- Identify the configuration type and format\n- Understand the scope and purpose\n- Note any environment-specific settings\n\nStep 2: Settings Review\n- Review all configuration parameters\n- Identify critical settings\n- Note any security-related configurations\n\nStep 3: Generate Summary\nAddress:\n1. Configuration file purpose\n2. Scope and environment context\n3. Critical settings and their impacts\n4. Security considerations\n5. Integration points\n\nFile: {file_path}\n\nConfiguration:\n{content}\n\nSummary:\"\"\",\n        \"key_points\": \"\"\"As a configuration expert, extract key configuration points following these steps:\n\nStep 1: Parameter Analysis\n- Identify all important parameters\n- Understand their purposes and impacts\n- Note any dependencies between settings\n\nStep 2: Critical Settings\n- List mission-critical configurations\n- Document default values and their implications\n- Note any security-sensitive settings\n\nStep 3: Generate Key Points\nFocus on:\n1. Essential configuration parameters\n2. Environment-specific settings\n3. Security configurations\n4. Integration parameters\n5. Performance-related settings\n\nFile: {file_path}\n\nConfiguration:\n{content}\n\nKey Configuration Points:\"\"\"\n    },\n    \"Documentation\": {\n        \"summary\": \"\"\"As a technical documentation expert, analyze this documentation following these steps:\n\nStep 1: Content Analysis\n- Understand the documentation scope\n- Identify main topics covered\n- Note any important guidelines or requirements\n\nStep 2: Documentation Review\n- Evaluate completeness and clarity\n- Identify key information sections\n- Note any technical specifications\n\nStep 3: Generate Summary\nAddress:\n1. Documentation purpose and scope\n2. Main topics covered\n3. Key guidelines or requirements\n4. Technical specifications\n5. Important usage examples\n\nFile: {file_path}\n\nDocumentation:\n{content}\n\nSummary:\"\"\",\n        \"key_points\": \"\"\"As a documentation specialist, extract key documentation points following these steps:\n\nStep 1: Content Review\n- Identify critical information\n- Note important guidelines\n- List key examples or demonstrations\n\nStep 2: Technical Details\n- Extract technical specifications\n- Note implementation requirements\n- List important references\n\nStep 3: Generate Key Points\nFocus on:\n1. Critical guidelines\n2. Technical requirements\n3. Important examples\n4. Best practices\n5. Key references\n\nFile: {file_path}\n\nDocumentation:\n{content}\n\nKey Documentation Points:\"\"\"\n    },\n    \"Build\": {\n        \"summary\": \"\"\"As a build system expert, analyze this build configuration following these steps:\n\nStep 1: Build Process Analysis\n- Understand build steps and dependencies\n- Identify build targets and artifacts\n- Note any special build requirements\n\nStep 2: Configuration Review\n- Review build settings and parameters\n- Identify critical dependencies\n- Note any platform-specific configurations\n\nStep 3: Generate Summary\nAddress:\n1. Build process overview\n2. Key targets and artifacts\n3. Critical dependencies\n4. Platform requirements\n5. Build optimization settings\n\nFile: {file_path}\n\nBuild Configuration:\n{content}\n\nSummary:\"\"\",\n        \"key_points\": \"\"\"As a build system specialist, extract key build points following these steps:\n\nStep 1: Build Configuration Analysis\n- Identify critical build steps\n- List important dependencies\n- Note build optimization settings\n\nStep 2: Platform Requirements\n- Document platform-specific settings\n- Note compatibility requirements\n- List required tools and versions\n\nStep 3: Generate Key Points\nFocus on:\n1. Critical build steps\n2. Essential dependencies\n3. Platform requirements\n4. Build optimizations\n5. Tool requirements\n\nFile: {file_path}\n\nBuild Configuration:\n{content}\n\nKey Build Points:\"\"\"\n    }\n}\n\n# --------------------------- Logging Setup --------------------------- #\n\nlogging.basicConfig(\n    filename=LOG_FILE,\n    filemode='a',\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    level=logging.INFO\n)\n\n# --------------------------- Model Worker Classes --------------------------- #\n\nclass ModelWorker:\n    \"\"\"\n    Handles model loading and text generation for file processing.\n    Compatible with Ray for distributed processing.\n    \"\"\"\n\n    def __init__(self, model_dir: str, hf_token: str):\n        \"\"\"\n        Initialize the ModelWorker with the allocated GPU.\n\n        Args:\n            model_dir (str): Path to the pre-trained model.\n            hf_token (str): Hugging Face authentication token.\n\n        Raises:\n            ValueError: If no GPUs are allocated or GPU ID is invalid.\n            Exception: For any other initialization errors.\n        \"\"\"\n        try:\n            # Retrieve the GPU IDs allocated by Ray\n            allocated_gpus = ray.get_gpu_ids()\n            if not allocated_gpus:\n                logging.error(\"No GPUs allocated to this worker. Exiting.\")\n                raise ValueError(\"No GPUs allocated to this worker.\")\n\n            gpu_id = allocated_gpus[0]\n\n            # Validate GPU ID\n            if gpu_id >= torch.cuda.device_count():\n                logging.error(f\"Allocated GPU ID {gpu_id} is invalid. Available GPUs: 0 to {torch.cuda.device_count()-1}\")\n                raise ValueError(f\"Invalid GPU ID: {gpu_id}\")\n\n            self.device = f'cuda:{gpu_id}'\n            logging.info(f\"ModelWorker initialized on GPU: {gpu_id}\")\n\n            # Initialize the tokenizer with updated parameter\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                model_dir,\n                token=hf_token,  # Replaced 'use_auth_token' with 'token'\n                trust_remote_code=True\n            )\n            self._init_model(model_dir, hf_token)\n        except Exception as e:\n            logging.error(f\"Error initializing ModelWorker: {e}\")\n            raise\n\n    def _init_model(self, model_dir: str, hf_token: str):\n        \"\"\"\n        Load the model onto the specified GPU.\n\n        Args:\n            model_dir (str): Path to the pre-trained model.\n            hf_token (str): Hugging Face authentication token.\n\n        Raises:\n            Exception: If model loading fails.\n        \"\"\"\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_dir,\n                trust_remote_code=True,\n                torch_dtype=torch.bfloat16 if self.device.startswith('cuda') else torch.float32,\n                token=hf_token  # Replaced 'use_auth_token' with 'token'\n            ).to(self.device)\n            self.model.eval()\n            logging.info(\"Model loaded successfully.\")\n        except Exception as e:\n            logging.error(f\"Error loading model: {e}\")\n            raise\n\n    def generate_text(self, prompt: str, chunk_size: int = CHUNK_SIZE) -> str:\n        \"\"\"\n        Generate text based on the provided prompt, handling chunking if necessary.\n\n        Args:\n            prompt (str): The input prompt for the model.\n            chunk_size (int): The maximum number of tokens per chunk.\n\n        Returns:\n            str: The generated text or an error message.\n        \"\"\"\n        try:\n            # Tokenize the prompt to get the token count\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=False)\n            token_count = len(inputs['input_ids'][0])\n\n            if token_count <= MAX_TOKENS:\n                return self._generate(prompt)\n\n            # If the prompt exceeds the max token limit, chunk it\n            return self._generate_chunked(prompt, chunk_size)\n        except Exception as e:\n            logging.error(f\"Error generating text: {e}\")\n            return f\"Error generating text: {str(e)}\"\n\n    def _generate_chunked(self, prompt: str, chunk_size: int) -> str:\n        \"\"\"\n        Generate text for chunked prompts and combine the results.\n\n        Args:\n            prompt (str): The input prompt exceeding MAX_TOKENS.\n            chunk_size (int): The maximum number of tokens per chunk.\n\n        Returns:\n            str: The combined and refined generated text.\n        \"\"\"\n        try:\n            chunks = []\n            remaining_text = prompt\n            while remaining_text:\n                # Tokenize and check the size\n                inputs = self.tokenizer(remaining_text, return_tensors=\"pt\", truncation=False)\n                token_count = len(inputs['input_ids'][0])\n\n                if token_count > chunk_size:\n                    # Find the approximate split point\n                    split_point = chunk_size\n                    chunk_ids = inputs['input_ids'][0][:split_point]\n                    chunk = self.tokenizer.decode(chunk_ids, skip_special_tokens=True)\n                    remaining_text = self.tokenizer.decode(inputs['input_ids'][0][split_point:], skip_special_tokens=True)\n                else:\n                    chunk = remaining_text\n                    remaining_text = \"\"\n\n                # Generate the output for each chunk\n                chunk_output = self._generate(chunk)\n                chunks.append(chunk_output)\n\n            # Combine the chunked outputs and refine the final output\n            combined = \" \".join(chunks)\n            refined_output = self._generate(f\"Please provide a coherent final version of this analysis:\\n{combined}\")\n            return refined_output\n        except Exception as e:\n            logging.error(f\"Error during chunked text generation: {e}\")\n            return f\"Error generating text: {str(e)}\"\n\n    def _generate(self, prompt: str) -> str:\n        \"\"\"\n        Generate text using the model based on the prompt.\n\n        Args:\n            prompt (str): The input prompt for the model.\n\n        Returns:\n            str: The generated text or an error message.\n        \"\"\"\n        try:\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n            attention_mask = inputs['attention_mask']  # Explicitly retrieve attention mask\n\n            with torch.no_grad():\n                output_ids = self.model.generate(\n                    inputs.input_ids,\n                    attention_mask=attention_mask,  # Pass attention mask\n                    max_length=MAX_TOKENS,\n                    num_return_sequences=1,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n\n            return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        except Exception as e:\n            logging.error(f\"Error during text generation: {e}\")\n            return f\"Error generating text: {str(e)}\"\n\n    def process_file(self, file_data: Dict) -> Dict:\n        \"\"\"\n        Process a single file to generate summary and key points.\n\n        Args:\n            file_data (Dict): A dictionary containing file information.\n\n        Returns:\n            Dict: A dictionary with the generated summary and key points.\n        \"\"\"\n        try:\n            file_path = file_data['file_path']\n            content = file_data['content']\n            category = file_data.get('Category', 'Unknown')\n\n            template = PROMPT_TEMPLATES.get(category, PROMPT_TEMPLATES[\"Documentation\"])\n\n            summary_prompt = template[\"summary\"].format(\n                file_path=file_path,\n                content=content\n            )\n            summary = self.generate_text(summary_prompt)\n\n            key_points_prompt = template[\"key_points\"].format(\n                file_path=file_path,\n                content=content\n            )\n            key_points = self.generate_text(key_points_prompt)\n\n            return {\n                'Category': category,\n                'file_path': file_path,\n                'summary': summary,\n                'key_points': key_points\n            }\n        except Exception as e:\n            logging.error(f\"Error processing file {file_data.get('file_path', 'Unknown')}: {e}\")\n            return {\n                'Category': file_data.get('Category', 'Unknown'),\n                'file_path': file_data.get('file_path', 'Unknown'),\n                'summary': f\"Error during processing: {str(e)}\",\n                'key_points': \"Processing failed\"\n            }\n\n# Ray-Compatible Distributed Model Worker\n@ray.remote(num_gpus=1)\nclass DistributedModelWorker(ModelWorker):\n    \"\"\"\n    Ray remote actor for distributed model processing.\n    Inherits from ModelWorker and initializes the model within Ray's context.\n    \"\"\"\n    def __init__(self, model_dir: str, hf_token: str):\n        super().__init__(model_dir, hf_token)\n\n# --------------------------- Ray Initialization --------------------------- #\n\ndef init_ray_cluster(required_gpus: int):\n    \"\"\"\n    Initialize Ray cluster with the required number of GPUs.\n\n    Args:\n        required_gpus (int): The number of GPUs needed.\n\n    Raises:\n        RuntimeError: If the available GPUs are less than required.\n    \"\"\"\n    if not ray.is_initialized():\n        try:\n            # Check available GPUs on the system\n            total_gpus = torch.cuda.device_count()\n            if total_gpus < required_gpus:\n                logging.error(f\"Requested {required_gpus} GPUs, but only {total_gpus} available.\")\n                raise RuntimeError(f\"Insufficient GPUs: Requested {required_gpus}, Available {total_gpus}\")\n\n            # Initialize Ray with 0 GPUs for the driver to reserve all GPUs for actors\n            ray.init(ignore_reinit_error=True, num_gpus=0)\n            logging.info(f\"Ray cluster initialized with {required_gpus} GPUs reserved for actors.\")\n        except Exception as e:\n            logging.error(f\"Error initializing Ray cluster: {e}\")\n            raise\n    else:\n        logging.info(\"Ray cluster is already initialized.\")\n\n# --------------------------- Documentation Generation --------------------------- #\n\ndef generate_documentation(results_df: pd.DataFrame) -> str:\n    \"\"\"\n    Generate comprehensive Markdown documentation from the results DataFrame.\n\n    Args:\n        results_df (pd.DataFrame): DataFrame containing processing results.\n\n    Returns:\n        str: The generated Markdown documentation.\n    \"\"\"\n    doc = \"# Comprehensive Codebase Documentation\\n\\n\"\n\n    for category in sorted(results_df['Category'].unique()):\n        doc += f\"## {category}\\n\\n\"\n        category_files = results_df[results_df['Category'] == category]\n\n        for _, row in category_files.iterrows():\n            doc += f\"### {row['file_path']}\\n\\n\"\n            doc += \"#### Summary\\n\\n\"\n            doc += f\"{row['summary']}\\n\\n\"\n            doc += \"#### Key Technical Points\\n\\n\"\n            doc += f\"{row['key_points']}\\n\\n\"\n            doc += \"---\\n\\n\"\n\n    return doc\n\n# --------------------------- Codebase Analysis --------------------------- #\n\ndef analyze_codebase(repo_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Analyze the codebase by processing each file and generating summaries.\n    Utilizes Ray for distributed GPU processing.\n\n    Args:\n        repo_df (pd.DataFrame): DataFrame containing files to process.\n\n    Returns:\n        pd.DataFrame: DataFrame with processing results.\n    \"\"\"\n    total_gpus = torch.cuda.device_count()\n    if total_gpus == 0:\n        logging.error(\"No GPUs available for processing. Exiting.\")\n        raise RuntimeError(\"No GPUs available for processing.\")\n\n    required_gpus = total_gpus\n    init_ray_cluster(required_gpus)\n\n    try:\n        # Initialize Ray workers based on available GPUs\n        workers = [\n            DistributedModelWorker.remote(MODEL_DIR, HF_TOKEN)\n            for _ in range(required_gpus)\n        ]\n        logging.info(f\"Initialized {len(workers)} Ray workers.\")\n\n        files = repo_df.to_dict('records')\n        num_files = len(files)\n        logging.info(f\"Total files to process: {num_files}\")\n\n        # Distribute files to workers in a round-robin fashion\n        futures = []\n        for idx, file_data in enumerate(files):\n            worker = workers[idx % len(workers)]\n            futures.append(worker.process_file.remote(file_data))\n\n        logging.info(\"Dispatching file processing tasks to Ray workers.\")\n        results = ray.get(futures)\n        logging.info(\"Completed file processing with Ray workers.\")\n\n        return pd.DataFrame(results)\n\n    except Exception as e:\n        logging.error(f\"Error during GPU processing with Ray: {e}\")\n        raise\n\n# --------------------------- Main Function --------------------------- #\n\ndef main(repo_df: pd.DataFrame) -> str:\n    \"\"\"\n    Main function to orchestrate codebase analysis and documentation generation.\n\n    Args:\n        repo_df (pd.DataFrame): DataFrame containing files to process.\n\n    Returns:\n        str: The generated Markdown documentation.\n    \"\"\"\n    try:\n        # Preprocessing the DataFrame: Filter out binary files\n        repo_df = repo_df[repo_df['is_binary'] == False].reset_index(drop=True)\n\n        if repo_df.empty:\n            logging.info(\"No valid files to process.\")\n            return \"No valid files to process.\"\n\n        # Analyze the codebase\n        results_df = analyze_codebase(repo_df)\n\n        # Validate 'Category' column\n        if 'Category' not in results_df.columns:\n            logging.error(\"'Category' column missing in results DataFrame.\")\n            raise KeyError(\"'Category' column is missing in the results.\")\n\n        # Generate documentation\n        documentation = generate_documentation(results_df)\n\n        # Write to output file\n        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n            f.write(documentation)\n\n        logging.info(f\"Documentation generated successfully: {OUTPUT_FILE}\")\n        return documentation\n\n    except KeyError as ke:\n        logging.error(f\"DataFrame KeyError: {ke}\")\n        sys.exit(f\"Error: {str(ke)}\")\n    except Exception as e:\n        logging.error(f\"Error in documentation generation: {e}\")\n        sys.exit(f\"An error occurred: {str(e)}\")\n    finally:\n        if ray.is_initialized():\n            ray.shutdown()\n            logging.info(\"Ray cluster shutdown.\")\n\n# --------------------------- Execution Entry Point --------------------------- #\n\nif __name__ == \"__main__\":\n    \n\n    # --------------------------- Execute Main Function --------------------------- #\n    try:\n        documentation = main(pre_patch_df.iloc[:20, :])\n        print(\"Documentation Generation Completed Successfully.\")\n        print(f\"Documentation saved to: {OUTPUT_FILE}\")\n    except Exception as e:\n        logging.error(f\"Unhandled exception: {e}\")\n        print(f\"An error occurred: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:16:34.507847Z","iopub.execute_input":"2024-12-30T19:16:34.508224Z"}},"outputs":[{"name":"stderr","text":"2024-12-30 19:16:36,799\tINFO worker.py:1753 -- Started a local Ray instance.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(autoscaler +6m7s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n\u001b[33m(autoscaler +6m7s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pre_patch_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.680139Z","iopub.status.idle":"2024-12-30T19:10:43.680639Z","shell.execute_reply.started":"2024-12-30T19:10:43.680380Z","shell.execute_reply":"2024-12-30T19:10:43.680406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for source code files in the dataframe\nsource_code_extensions = ['.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.c', '.cpp', '.h', '.hpp', '.rb', '.go', '.sh']\nsource_code_df = pre_patch_df[pre_patch_df['file_path'].str.endswith(tuple(source_code_extensions))]\nprint(f\"Number of source code files: {len(source_code_df)}\")\nprint(source_code_df[['file_path', 'Category']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.682243Z","iopub.status.idle":"2024-12-30T19:10:43.682730Z","shell.execute_reply.started":"2024-12-30T19:10:43.682494Z","shell.execute_reply":"2024-12-30T19:10:43.682519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repo_dataframe['file_path'] = repo_dataframe['file_path'].apply(os.path.normpath)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.684158Z","iopub.status.idle":"2024-12-30T19:10:43.684653Z","shell.execute_reply.started":"2024-12-30T19:10:43.684401Z","shell.execute_reply":"2024-12-30T19:10:43.684426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Knowledge Graph","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport os\nimport re\nimport logging\nfrom functools import lru_cache\nfrom typing import List\nimport json\nimport ast\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom matplotlib.patches import Patch\nimport numpy as np\n\n# Configure logging to DEBUG for detailed output\nlogging.basicConfig(level=logging.DEBUG, format='%(levelname)s:%(message)s')\n\nclass KnowledgeGraph:\n    def __init__(self, dataframe: pd.DataFrame):\n        self.df = dataframe.copy()\n        # Normalize all file paths to ensure consistency\n        self.df['file_path'] = self.df['file_path'].apply(os.path.normpath)\n        self.graph = nx.DiGraph()\n        self.required_columns = {'file_path', 'content', 'is_binary', 'Category'}\n        self.validate_dataframe()\n        self.add_nodes()\n        self.cache_file_content()\n    \n    def validate_dataframe(self):\n        if not self.required_columns.issubset(self.df.columns):\n            missing = self.required_columns - set(self.df.columns)\n            logging.error(f\"Dataframe is missing required columns: {missing}\")\n            raise ValueError(f\"Dataframe must contain columns: {self.required_columns}\")\n        logging.info(\"Dataframe loaded successfully with all required columns.\")\n    \n    def add_nodes(self):\n        for _, row in self.df.iterrows():\n            try:\n                self.graph.add_node(row['file_path'], category=row['Category'])\n                logging.debug(f\"Added node: {row['file_path']} with category: {row['Category']}\")\n            except Exception as e:\n                logging.error(f\"Error adding node for file {row['file_path']}: {e}\")\n    \n    @lru_cache(maxsize=None)\n    def get_file_content(self, file_path: str) -> str:\n        try:\n            content = self.df.loc[self.df['file_path'] == file_path, 'content'].values[0]\n            return content\n        except IndexError:\n            logging.warning(f\"File content not found for {file_path}.\")\n            return \"\"\n    \n    def extract_dependencies(self, file_path: str, content: str, category: str) -> List[str]:\n        dependencies = []\n        try:\n            if category == 'Source Code Files':\n                if file_path.endswith('.py'):\n                    dependencies.extend(self.extract_python_dependencies(file_path, content))\n                elif file_path.endswith('.js') or file_path.endswith('.jsx'):\n                    dependencies.extend(self.extract_javascript_dependencies(file_path, content))\n                elif file_path.endswith('.ts') or file_path.endswith('.tsx'):\n                    dependencies.extend(self.extract_typescript_dependencies(file_path, content))\n                elif file_path.endswith('.java'):\n                    dependencies.extend(self.extract_java_dependencies(file_path, content))\n                elif file_path.endswith(('.cpp', '.c', '.hpp', '.h')):\n                    dependencies.extend(self.extract_cpp_dependencies(file_path, content))\n                elif file_path.endswith('.rb'):\n                    dependencies.extend(self.extract_ruby_dependencies(file_path, content))\n                elif file_path.endswith('.go'):\n                    dependencies.extend(self.extract_go_dependencies(file_path, content))\n                # Add more languages as needed\n            elif category == 'Testing Files':\n                if file_path.endswith('.py'):\n                    dependencies.extend(self.extract_python_dependencies(file_path, content))\n                elif file_path.endswith('.js') or file_path.endswith('.jsx'):\n                    dependencies.extend(self.extract_javascript_dependencies(file_path, content))\n                # Add more languages as needed\n            elif category == 'Scripts and Utilities':\n                dependencies.extend(self.extract_shell_dependencies(file_path, content))\n            elif category == 'Documentation Files':\n                dependencies.extend(self.extract_markdown_assets(file_path, content))\n            elif category == 'Configuration Files':\n                dependencies.extend(self.extract_config_dependencies(file_path, content))\n            elif category == 'Workflow and CI Files':\n                dependencies.extend(self.extract_workflow_dependencies(file_path, content))\n            # Add more categories and their extraction functions as needed\n        except Exception as e:\n            logging.error(f\"Error extracting dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    # Dependency extraction methods for various languages and file types\n    \n    def extract_python_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            tree = ast.parse(content)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        module = alias.name.split('.')[0]\n                        dep_file = os.path.normpath(os.path.join(os.path.dirname(file_path), f\"{module}.py\"))\n                        if dep_file in self.df['file_path'].values:\n                            dependencies.append(dep_file)\n                            logging.debug(f\"Python dependency found: {dep_file} imported in {file_path}\")\n                elif isinstance(node, ast.ImportFrom):\n                    module = node.module.split('.')[0] if node.module else ''\n                    if module:\n                        dep_file = os.path.normpath(os.path.join(os.path.dirname(file_path), f\"{module}.py\"))\n                        if dep_file in self.df['file_path'].values:\n                            dependencies.append(dep_file)\n                            logging.debug(f\"Python dependency found: {dep_file} imported in {file_path}\")\n        except SyntaxError as se:\n            logging.warning(f\"Syntax error while parsing {file_path}: {se}\")\n        except Exception as e:\n            logging.error(f\"Unexpected error while parsing {file_path}: {e}\")\n        return dependencies\n    \n    def extract_javascript_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # ES6 import statements: import something from './module.js'\n            pattern = r'import\\s+.*\\s+from\\s+[\\'\"](.+?\\.js)[\\'\"]'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                dep_path = os.path.normpath(os.path.join(os.path.dirname(file_path), match))\n                if dep_path in self.df['file_path'].values:\n                    dependencies.append(dep_path)\n                    logging.debug(f\"JavaScript dependency found: {dep_path} imported in {file_path}\")\n            # CommonJS require statements: const module = require('./module.js')\n            pattern_cjs = r'require\\([\\'\"](.+?\\.js)[\\'\"]\\)'\n            matches_cjs = re.findall(pattern_cjs, content)\n            for match in matches_cjs:\n                dep_path = os.path.normpath(os.path.join(os.path.dirname(file_path), match))\n                if dep_path in self.df['file_path'].values:\n                    dependencies.append(dep_path)\n                    logging.debug(f\"JavaScript dependency found: {dep_path} required in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting JavaScript dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_typescript_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # ES6 import statements: import something from './module.ts'\n            pattern = r'import\\s+.*\\s+from\\s+[\\'\"](.+?\\.ts)[\\'\"]'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                dep_path = os.path.normpath(os.path.join(os.path.dirname(file_path), match))\n                if dep_path in self.df['file_path'].values:\n                    dependencies.append(dep_path)\n                    logging.debug(f\"TypeScript dependency found: {dep_path} imported in {file_path}\")\n            # CommonJS require statements: const module = require('./module.ts')\n            pattern_cjs = r'require\\([\\'\"](.+?\\.ts)[\\'\"]\\)'\n            matches_cjs = re.findall(pattern_cjs, content)\n            for match in matches_cjs:\n                dep_path = os.path.normpath(os.path.join(os.path.dirname(file_path), match))\n                if dep_path in self.df['file_path'].values:\n                    dependencies.append(dep_path)\n                    logging.debug(f\"TypeScript dependency found: {dep_path} required in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting TypeScript dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_java_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Java import statements: import com.example.Module;\n            pattern = r'import\\s+([a-zA-Z0-9_.]+);'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                module = match.split('.')[-1]\n                dep_file = os.path.normpath(os.path.join(os.path.dirname(file_path), f\"{module}.java\"))\n                if dep_file in self.df['file_path'].values:\n                    dependencies.append(dep_file)\n                    logging.debug(f\"Java dependency found: {dep_file} imported in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting Java dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_cpp_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # C/C++ include statements: #include \"module.h\"\n            pattern = r'#include\\s+[<\"](.+?\\.h)[>\"]'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                dep_path = os.path.normpath(os.path.join(os.path.dirname(file_path), match))\n                if dep_path in self.df['file_path'].values:\n                    dependencies.append(dep_path)\n                    logging.debug(f\"C/C++ dependency found: {dep_path} included in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting C/C++ dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_ruby_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Ruby require statements: require 'module'\n            pattern = r'require\\s+[\\'\"](.+?)[\\'\"]'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                dep_file = os.path.normpath(os.path.join(os.path.dirname(file_path), f\"{match}.rb\"))\n                if dep_file in self.df['file_path'].values:\n                    dependencies.append(dep_file)\n                    logging.debug(f\"Ruby dependency found: {dep_file} required in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting Ruby dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_go_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Go import statements: import \"github.com/user/module\"\n            pattern = r'import\\s+\"([^\"]+)\"'\n            matches = re.findall(pattern, content)\n            for match in matches:\n                module = match.split('/')[-1]\n                dep_file = os.path.normpath(os.path.join(os.path.dirname(file_path), f\"{module}.go\"))\n                if dep_file in self.df['file_path'].values:\n                    dependencies.append(dep_file)\n                    logging.debug(f\"Go dependency found: {dep_file} imported in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting Go dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_shell_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Look for 'docker build' commands referencing Dockerfile\n            if 'docker build' in content:\n                dockerfile = os.path.normpath(os.path.join(os.path.dirname(file_path), 'Dockerfile'))\n                if dockerfile in self.df['file_path'].values:\n                    dependencies.append(dockerfile)\n                    logging.debug(f\"Shell dependency found: {dockerfile} referenced in {file_path}\")\n            # Sourcing other scripts: source scripts/helper.sh\n            sourced_scripts = re.findall(r'source\\s+(.+?\\.sh)', content)\n            for script in sourced_scripts:\n                script_path = os.path.normpath(os.path.join(os.path.dirname(file_path), script))\n                if script_path in self.df['file_path'].values:\n                    dependencies.append(script_path)\n                    logging.debug(f\"Shell dependency found: {script_path} sourced in {file_path}\")\n            # Executing other scripts: ./scripts/setup.sh\n            executed_scripts = re.findall(r'\\./(.+?\\.sh)', content)\n            for script in executed_scripts:\n                script_path = os.path.normpath(os.path.join(os.path.dirname(file_path), script))\n                if script_path in self.df['file_path'].values:\n                    dependencies.append(script_path)\n                    logging.debug(f\"Shell dependency found: {script_path} executed in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting shell dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_markdown_assets(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Regex to find image links: ![Alt Text](assets/image.png)\n            pattern = r'!\\[.*?\\]\\((assets/[^)]+\\.(png|jpg|jpeg|gif|svg))\\)'\n            matches = re.findall(pattern, content, re.IGNORECASE)\n            for match in matches:\n                asset_path = os.path.normpath(match[0])\n                if asset_path in self.df['file_path'].values:\n                    dependencies.append(asset_path)\n                    logging.debug(f\"Markdown asset found: {asset_path} referenced in {file_path}\")\n            # Regex to find other asset references, e.g., scripts or styles\n            pattern_assets = r'\\((assets/[^)]+)\\)'\n            matches_assets = re.findall(pattern_assets, content, re.IGNORECASE)\n            for asset in matches_assets:\n                asset_path = os.path.normpath(asset)\n                if asset_path in self.df['file_path'].values:\n                    dependencies.append(asset_path)\n                    logging.debug(f\"Markdown asset found: {asset_path} referenced in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting Markdown assets from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_config_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Parse JSON configuration files\n            if file_path.endswith('.json'):\n                try:\n                    config = json.loads(content)\n                    # Example: Look for file references in specific keys\n                    # Modify based on actual config structure\n                    # For demonstration, assume 'scripts' key contains script paths\n                    scripts = config.get('scripts', {})\n                    for script_path in scripts.values():\n                        script_path = os.path.normpath(script_path)\n                        if script_path in self.df['file_path'].values:\n                            dependencies.append(script_path)\n                            logging.debug(f\"Config dependency found: {script_path} referenced in {file_path}\")\n                except json.JSONDecodeError:\n                    logging.warning(f\"JSON decode error in {file_path}\")\n            # Parse YAML configuration files\n            elif file_path.endswith(('.yml', '.yaml')):\n                try:\n                    import yaml\n                    config = yaml.safe_load(content)\n                    # Example: Look for file references in specific keys\n                    scripts = config.get('scripts', {})\n                    for script_path in scripts.values():\n                        script_path = os.path.normpath(script_path)\n                        if script_path in self.df['file_path'].values:\n                            dependencies.append(script_path)\n                            logging.debug(f\"Config dependency found: {script_path} referenced in {file_path}\")\n                except ImportError:\n                    logging.error(\"PyYAML is not installed. Install it using 'pip install pyyaml'\")\n                except yaml.YAMLError:\n                    logging.warning(f\"YAML parse error in {file_path}\")\n            # Add more configuration file types as needed\n        except Exception as e:\n            logging.error(f\"Error extracting configuration dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def extract_workflow_dependencies(self, file_path: str, content: str) -> List[str]:\n        dependencies = []\n        try:\n            # Look for 'run: scripts/deploy.sh' or similar\n            run_scripts = re.findall(r'run:\\s*(?:bash\\s+)?(.+?\\.sh)', content)\n            for script in run_scripts:\n                script_path = os.path.normpath(os.path.join(os.path.dirname(file_path), script))\n                if script_path in self.df['file_path'].values:\n                    dependencies.append(script_path)\n                    logging.debug(f\"Workflow dependency found: {script_path} run in {file_path}\")\n            \n            # Look for test scripts (e.g., pytest)\n            test_scripts = re.findall(r'run:\\s*pytest\\s+(.+)', content)\n            for test_script in test_scripts:\n                # Assuming tests are in 'tests/' directory or similar\n                test_file = os.path.normpath(os.path.join(os.path.dirname(file_path), test_script.strip()))\n                if test_file in self.df['file_path'].values:\n                    dependencies.append(test_file)\n                    logging.debug(f\"Workflow dependency found: {test_file} tested in {file_path}\")\n        except Exception as e:\n            logging.error(f\"Error extracting workflow dependencies from {file_path}: {e}\")\n        return dependencies\n    \n    def build_edges_sequential(self):\n        \"\"\"\n        Builds edges in the graph based on dependencies sequentially.\n        This method replaces the multiprocessing approach for easier debugging.\n        \"\"\"\n        logging.info(\"Starting sequential edge building.\")\n        for idx, row in self.df.iterrows():\n            file_path = row['file_path']\n            content = row['content']\n            category = row['Category']\n            if row['is_binary']:\n                logging.debug(f\"Skipping binary file: {file_path}\")\n                continue  # Skip binary files\n            dependencies = self.extract_dependencies(file_path, content, category)\n            logging.debug(f\"Dependencies for {file_path}: {dependencies}\")\n            for dep in dependencies:\n                if dep in self.df['file_path'].values:\n                    self.graph.add_edge(file_path, dep, relationship='DEPENDS_ON')\n                    logging.debug(f\"Added edge: {file_path} DEPENDS_ON {dep}\")\n                else:\n                    logging.warning(f\"Dependency {dep} for file {file_path} not found in dataframe.\")\n        logging.info(\"Completed sequential edge building.\")\n    \n    def cache_file_content(self):\n        \"\"\"\n        Caches file content to optimize repeated access.\n        Currently implemented using lru_cache decorator on get_file_content.\n        \"\"\"\n        # This method can be expanded if needed\n        pass\n    \n    def get_graph_dataframes(self):\n        \"\"\"\n        Converts the NetworkX graph into pandas DataFrames for nodes and edges.\n        \n        Returns:\n            nodes_df (pd.DataFrame): DataFrame containing node information.\n            edges_df (pd.DataFrame): DataFrame containing edge information.\n        \"\"\"\n        # Extract nodes with attributes\n        nodes_data = []\n        for node, attrs in self.graph.nodes(data=True):\n            node_entry = {'file_path': node}\n            node_entry.update(attrs)\n            nodes_data.append(node_entry)\n        nodes_df = pd.DataFrame(nodes_data)\n        \n        # Extract edges with attributes\n        edges_data = []\n        for source, target, attrs in self.graph.edges(data=True):\n            edge_entry = {\n                'source': source,\n                'target': target,\n                'relationship': attrs.get('relationship', '')\n            }\n            edges_data.append(edge_entry)\n        edges_df = pd.DataFrame(edges_data)\n        \n        return nodes_df, edges_df\n\n\n# Initialize KnowledgeGraph with sample data\nkg = KnowledgeGraph(repo_dataframe)\n\n# Build edges sequentially\nkg.build_edges_sequential()\n\n# Convert graph to DataFrames\nnodes_df, edges_df = kg.get_graph_dataframes()\n\n# Display the DataFrames\nprint(\"Nodes DataFrame:\")\nprint(nodes_df)\nprint(\"\\nEdges DataFrame:\")\nprint(edges_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.686925Z","iopub.status.idle":"2024-12-30T19:10:43.687416Z","shell.execute_reply.started":"2024-12-30T19:10:43.687151Z","shell.execute_reply":"2024-12-30T19:10:43.687175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef create_knowledge_graph(nodes_df, edges_df, figsize=(20, 16)):\n    # Create a NetworkX graph\n    G = nx.DiGraph()\n    \n    # Add nodes with category as an attribute\n    for _, row in nodes_df.iterrows():\n        G.add_node(row['file_path'], category=row['category'])\n    \n    # Add edges with relationship as an attribute\n    for _, row in edges_df.iterrows():\n        G.add_edge(row['source'], row['target'], relationship=row['relationship'])\n    \n    # Define a visually distinct color map for different categories\n    categories = sorted(nodes_df['category'].unique())\n    color_map = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n    category_colors = {category: color_map[i] for i, category in enumerate(categories)}\n    \n    # Assign colors to nodes based on their category\n    node_colors = [category_colors[G.nodes[node]['category']] for node in G.nodes()]\n    \n    # Create figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Use a force-directed layout with optimized parameters for spacing\n    pos = nx.spring_layout(\n        G,\n        k=1.5/np.sqrt(len(G.nodes())),  # Optimal distance between nodes\n        iterations=50,  # More iterations for better convergence\n        seed=42  # For reproducibility\n    )\n    \n    # Draw nodes with enhanced visibility\n    nodes = nx.draw_networkx_nodes(\n        G, pos,\n        node_color=node_colors,\n        node_size=2000,  # Larger nodes\n        alpha=0.7,\n        edgecolors='white',  # White border for better contrast\n        linewidths=2\n    )\n    \n    # Draw edges with improved styling\n    edges = nx.draw_networkx_edges(\n        G, pos,\n        edge_color='gray',\n        arrowsize=20,\n        arrowstyle='->',\n        width=2,\n        alpha=0.6,\n        connectionstyle='arc3,rad=0.2'  # Curved edges for better visibility\n    )\n    \n    # Add labels with improved readability\n    labels = nx.draw_networkx_labels(\n        G, pos,\n        font_size=10,\n        font_weight='bold',\n        font_family='sans-serif',\n        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=4.0)\n    )\n    \n    # Create a custom legend\n    legend_elements = [Patch(facecolor=color, label=cat, alpha=0.7)\n                      for cat, color in category_colors.items()]\n    ax.legend(\n        handles=legend_elements,\n        title='Categories',\n        title_fontsize=12,\n        fontsize=10,\n        loc='center left',\n        bbox_to_anchor=(1, 0.5),\n        frameon=True,\n        facecolor='white',\n        edgecolor='gray'\n    )\n    \n    # Add title and styling\n    plt.title(\n        \"Knowledge Graph Visualization\",\n        pad=20,\n        fontsize=16,\n        fontweight='bold'\n    )\n    \n    # Remove axes and add padding\n    plt.axis('off')\n    plt.tight_layout(pad=2.0)\n    \n    return fig, ax\n\n# Example usage:\nfig, ax = create_knowledge_graph(nodes_df, edges_df, figsize=(34, 30))\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.688490Z","iopub.status.idle":"2024-12-30T19:10:43.688926Z","shell.execute_reply.started":"2024-12-30T19:10:43.688698Z","shell.execute_reply":"2024-12-30T19:10:43.688721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nodes_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.690330Z","iopub.status.idle":"2024-12-30T19:10:43.690769Z","shell.execute_reply.started":"2024-12-30T19:10:43.690543Z","shell.execute_reply":"2024-12-30T19:10:43.690566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edges_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:10:43.692092Z","iopub.status.idle":"2024-12-30T19:10:43.692554Z","shell.execute_reply.started":"2024-12-30T19:10:43.692314Z","shell.execute_reply":"2024-12-30T19:10:43.692340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}